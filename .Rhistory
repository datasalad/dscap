knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/dscapstone/")
library(tidyverse)
samplesFiltered <- readRDS("filtered.rds")
t <- tbl_df(samplesFiltered)
t
nrow(t[t$sanity_failed,])
t[t$sanity_failed,]
f <- filter(t, sanity_failed == TRUE)
f
f <- filter(t, sanity_failed == FALSE)
f
samplesFiltered <- readRDS("filtered.rds")
t <- tbl_df(samplesFiltered)
nrow(t[t$sanity_failed,])
t
filter(t, sanity_failed == TRUE)
gsub("btch", "8", "hello btch how are you btch")
nrow(t)
dim(t)
names(t)
t[1,2]
t[1,1]
filter <- read.csv("sanityfilter_en.txt", header = FALSE, sep = "\n", as.is = TRUE)$V1
f1 <- c()
for (i in 1:length(filter)) {
## \\<word\\>
word <- paste("\\<", filter[i] ,"\\>", sep = "")
f1 <- append(f1, word)
}
f <- paste(f1, sep = "", collapse = "|")
t <- removeBadWords(t, paste("(", f, ")", sep = ""))
removeBadWords <- function(t, pattern) {
for (i in 1:nrow(t)) {
if (t[i, 2] == TRUE) { ## sanity failed
t[i,1] <- gsub(pattern, "*", t[i,1])
}
}
}
t <- removeBadWords(t, paste("(", f, ")", sep = ""))
t
t <- tbl_df(samplesFiltered)
t
removeBadWords <- function(t, pattern) {
for (i in 1:nrow(t)) {
if (t[i, 2] == TRUE) { ## sanity failed
print(t[i,1])
t[i,1] <- gsub(pattern, "*", t[i,1])
print("======= >")
print(t[i,1])
}
}
}
t <- removeBadWords(t, paste("(", f, ")", sep = ""))
t <- removeBadWords(t, paste("(", f, ")", sep = ""))
removeBadWords <- function(t, pattern) {
for (i in 1:nrow(t)) {
if (t[i, 2] == TRUE) { ## sanity failed
t[i,1] <- gsub(pattern, "*", t[i,1])
}
}
t
}
t <- removeBadWords(t, paste("(", f, ")", sep = ""))
t
dd <- filter(t, sanity_failed == TRUE)
dd
t
saveRDS(t, "preprocessed_needs_rm_spec_chars.rds")
?t.test
a <- c(1, 2, 3)
b <- c(6, 8, 10)
t.test(a,b)
t.test(a,b, conf.level = 0.9)
t.test(a,b, conf.level = 0.9)$p.v
t.test(a,b, conf.level = 0.9)$p.v < 0.05
t.test(a,b, conf.level = 0.9, paired = TRUE)$p.v < 0.05
t.test(a,b, conf.level = 0.9, paired = TRUE)
dtm <- TermDocumentMatrix(docs)
library(tm)
library(Rgraphviz)
library(tidyverse)
preprocessed <- readRDS("preprocessed_needs_rm_spec_chars.rds")
t <- tbl_df(preprocessed)
nrow(t[t$sanity_failed,])
head(filter(t, sanity_failed == TRUE)$value, 5)
docs <- Corpus(VectorSource(t$value))
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
docs
docs
docs[1]
docs[[1]
]
observe(docs)
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
dtm
dtm$dimnames
dtm$Terms
?VectorSource
docs <- data.frame(doc_id = c("en_US.blogs.txt", "en_US.news.txt", "en_US.twitter.txt"),
text = t$value,
stringsAsFactors = FALSE)
вщсы
docs
(ds <- DataframeSource(docs))
(ds <- DataframeSource(docs))
x <- Corpus(ds)
ч
x
inspect(x)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tm)
library(RWeka)
t <- tbl_df(readRDS("preprocessed_needs_rm_spec_chars.rds"))
nrow(t[t$sanity_failed,])
corp <- Corpus(VectorSource(t$value))
corp <- tm_map(corp, content_transformer(function(x, pattern) gsub(pattern, " ", x)),  "“|”|–|…|‘|’")
corp <- tm_map(corp, content_transformer(removePunctuation))
corp <- tm_map(corp, content_transformer(removeNumbers))
corp <- tm_map(corp, stripWhitespace)
ng <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ng
matr <- TermDocumentMatrix(corp, control = list(tokenize = ng))
matr
inspect(matr)
corp <- VCorpus(VectorSource(t$value))
corp
corp <- tm_map(corp, content_transformer(function(x, pattern) gsub(pattern, " ", x)),  "“|”|–|…|‘|’")
corp <- tm_map(corp, content_transformer(function(x, pattern) gsub(pattern, " ", x)),  "“|”|–|…|‘|’")
corp <- tm_map(corp, content_transformer(removePunctuation))
corp <- tm_map(corp, content_transformer(removeNumbers))
corp <- tm_map(corp, stripWhitespace)
ngramtoken1 <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
ngramtoken2 <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
ngramtoken3 <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
ngrammatr1 <- TermDocumentMatrix(corp, control = list(tokenize = ngramtoken1))
?reorder
preproc <- NULL
preproc$a <- 1
preproc$b <- 1
preproc$c <- c(1,23)
preproc
preproc$d <- data.frame(d = 1)
preproc
test <- TermDocumentMatrix()
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(RWeka)
library(tidyverse)
test <- TermDocumentMatrix()
?TermDocumentMatrix
install.packages("topicmodels")
install.packages("ANLP")
preprocessed <- readRDS("preprocessed.rds")
ns <- removeSparseTerms(preprocessed$ngrammatr2, sparse=0.3)
ns
td <- as.matrix(ns)
> distMatrix <- dist(scale(td))
distMatrix <- dist(scale(td))
fit <- hclust(distMatrix, method="ward.D2")
fit <- hclust(distMatrix)
ns <- removeSparseTerms(preprocessed$ngrammatr2, sparse=0.8)
td <- as.matrix(ns)
distMatrix <- dist(scale(td))
fit <- hclust(distMatrix, method="ward.D2")
distMatrix
distMatrix <- dist(scale(preprocessed$ngrammatr2))
ns <- removeSparseTerms(preprocessed$ngrammatr2)
?removeSparseTerms
ns <- removeSparseTerms(preprocessed$ngrammatr2, 0.2)
distMatrix <- dist(scale(as.matrix(ns)))
distMatrix
distMatrix[0]
findAssocs
?findAssocs
findAssocs(preprocessed$ngrammatr2, "test")
findAssocs(preprocessed$ngrammatr2, "test", 0.5)
findAssocs(preprocessed$ngrammatr2, "test", 0.1)
findAssocs(preprocessed$ngrammatr1, "test", 0.1)
findAssocs(preprocessed$ngrammatr1, "test", 0.6)
tdm <- removeSparseTerms(preprocessed$ngrammatr2, 0.9)
findAssocs(tdm, "test", 0.6)
findAssocs(tdm, "test", 0.1)
findAssocs(tdm, "test", 0.0001)
findAssocs(tdm, "war", 0.0001)
findAssocs(tdm, "war")
findAssocs(tdm, "war", 0)
inspect(tdm)
findAssocs(tdm, "in", 0)
findAssocs(tdm, "in the", 0)
findAssocs(tdm, "the", 0)
findFreqTerms(tdm)
class(preprocessed$ngrammatr2)
findFreqTerms(preprocessed$ngrammatr2)
findMostTerms(preprocessed$ngrammatr2)
findMostFreqTerms(preprocessed$ngrammatr2)
install.packages(c("knitr", "tidyverse", "tm", "RWeka"))
install.packages(c("knitr", "tidyverse", "tm", "RWeka"))
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
rm(list=ls9)
rm(list=ls())
shiny::runApp('Desktop/shiny/ddp_week4')
?textInput
runApp('Desktop/dscap/ShinAppDsCapstone')
?actionButton
runApp('Desktop/dscap/ShinAppDsCapstone')
?observeEvent
runApp('Desktop/dscap/ShinAppDsCapstone')
library(rsconnect)
rsconnect::deployApp('Desktop/dscap/ShinAppDsCapstone')
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Desktop/dscap/")
db <- readRDS("dbopt10.rds")
db
library(tidyverse)
dbOpt <- function(full) {
t <- tbl_df(full)
opt <- filter(t, freq >= 25)
d <- data.table(feat = opt$feat, freq = opt$freq)
d
}
dbOp <- NULL
dbOp$n1 <- dbOpt(db$n1)
dbOp$n2 <- dbOpt(db$n2)
dbOp$n3 <- dbOpt(db$n3)
dbOp$n4 <- dbOpt(db$n4)
dbOp$n5 <- dbOpt(db$n5)
dbOp$n6 <- dbOpt(db$n6)
saveRDS(dbOp, "dbopt25.rds")
db <- readRDS("dbopt25.rds")
getwd()
setwd("~/Desktop/dscap/")
db <- readRDS("dbopt25.rds")
setwd("~/Desktop/dscap//")
db <- readRDS("dbopt25.rds")
db
p <- "let"
while(TRUE) { p <- paste0(paste0(" ", p), " ", predictWord(p)); print(p) }
rsconnect::deployApp('Desktop/dscap/ShinAppDsCapstone')
runApp('ShinAppDsCapstone')
getwd()
library(rsconnect)
rsconnect::deployApp("/Users/usr/Desktop/dscap/ShinAppDsCapstone/")
